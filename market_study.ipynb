{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03844570",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kshitij\\OneDrive\\Desktop\\RozReturns\\market_regime\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from hdbscan import HDBSCAN\n",
    "import umap\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.colors import ListedColormap\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bebf4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f5e7799",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_depth_data(file_path):\n",
    "    \"\"\"Load orderâ€‘book depth data from file, parsing the IST timestamps.\"\"\"\n",
    "    print(f\"Loading depth data from {file_path}...\")\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # 1) Remove the trailing \" IST\"\n",
    "    # 2) Parse with pandas (it handles the \"+0530\" offset and nanoseconds)\n",
    "    df['Time'] = (\n",
    "        df['Time']\n",
    "          .str.replace(r'\\s+IST$', '', regex=True)\n",
    "          .pipe(pd.to_datetime, utc=True)   # becomes UTC; you can .dt.tz_convert('Asia/Kolkata') if you want local time\n",
    "    )\n",
    "    df.set_index('Time', inplace=True)\n",
    "    return df\n",
    "\n",
    "def load_trade_data(file_path):\n",
    "    \"\"\"Load trade data from file, parsing the IST timestamps.\"\"\"\n",
    "    print(f\"Loading trade data from {file_path}...\")\n",
    "    df = pd.read_csv(file_path)\n",
    "    df['Time'] = (\n",
    "        df['Time']\n",
    "          .str.replace(r'\\s+IST$', '', regex=True)\n",
    "          .pipe(pd.to_datetime, utc=True)\n",
    "    )\n",
    "    df.set_index('Time', inplace=True)\n",
    "    return df\n",
    "\n",
    "def load_all_data(base_path, date_range):\n",
    "    \"\"\"Load all depth and trade data for given date range.\"\"\"\n",
    "    depth_data = {}\n",
    "    trade_data = {}\n",
    "    \n",
    "    for date in date_range:\n",
    "        date_str = date.strftime('%Y%m%d')\n",
    "        depth_file = os.path.join(base_path, 'depth20_1000ms', f'BNBFDUSD_{date_str}.txt')\n",
    "        trade_file = os.path.join(base_path, 'aggTrade',      f'BNBFDUSD_{date_str}.txt')\n",
    "        \n",
    "        if os.path.exists(depth_file):\n",
    "            depth_data[date_str] = load_depth_data(depth_file)\n",
    "        \n",
    "        if os.path.exists(trade_file):\n",
    "            trade_data[date_str] = load_trade_data(trade_file)\n",
    "    \n",
    "    return depth_data, trade_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da0e9c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define date range for analysis (March 14-17, 2025)\n",
    "date_range = [datetime(2025, 3, 14) + timedelta(days=i) for i in range(4)]\n",
    "\n",
    "# Update with your actual data path\n",
    "base_path = \"./data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1505f7c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading depth data from ./data\\depth20_1000ms\\BNBFDUSD_20250314.txt...\n",
      "Loading trade data from ./data\\aggTrade\\BNBFDUSD_20250314.txt...\n",
      "Loading depth data from ./data\\depth20_1000ms\\BNBFDUSD_20250315.txt...\n",
      "Loading trade data from ./data\\aggTrade\\BNBFDUSD_20250315.txt...\n",
      "Loading depth data from ./data\\depth20_1000ms\\BNBFDUSD_20250316.txt...\n",
      "Loading trade data from ./data\\aggTrade\\BNBFDUSD_20250316.txt...\n",
      "Loading depth data from ./data\\depth20_1000ms\\BNBFDUSD_20250317.txt...\n",
      "Loading trade data from ./data\\aggTrade\\BNBFDUSD_20250317.txt...\n"
     ]
    }
   ],
   "source": [
    "depth_data, trade_data = load_all_data(base_path, date_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ec61280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating synthetic data for demonstration...\n"
     ]
    }
   ],
   "source": [
    "def create_synthetic_data():\n",
    "    \"\"\"Create synthetic data for demonstration.\"\"\"\n",
    "    print(\"Creating synthetic data for demonstration...\")\n",
    "    \n",
    "    # Create timestamps for 4 days, every second\n",
    "    start_date = datetime(2025, 3, 14)\n",
    "    dates = {}\n",
    "    trade_dates = {}\n",
    "    \n",
    "    for i in range(4):\n",
    "        date = start_date + timedelta(days=i)\n",
    "        date_str = date.strftime('%Y%m%d')\n",
    "        \n",
    "        # Create timestamps for depth data (every second)\n",
    "        timestamps = [date + timedelta(seconds=j) for j in range(86400)]  # 24 hours\n",
    "        \n",
    "        # Create synthetic depth data\n",
    "        depth_df = pd.DataFrame(index=timestamps)\n",
    "        \n",
    "        # Create bid columns\n",
    "        for level in range(1, 21):\n",
    "            base_price = 605.0 + np.random.normal(0, 0.5)\n",
    "            depth_df[f'BidPriceL{level}'] = [base_price - 0.01*level + np.random.normal(0, 0.02) for _ in range(len(timestamps))]\n",
    "            depth_df[f'BidQtyL{level}'] = [np.random.exponential(2) for _ in range(len(timestamps))]\n",
    "        \n",
    "        # Create ask columns\n",
    "        for level in range(1, 21):\n",
    "            base_price = 605.2 + np.random.normal(0, 0.5)\n",
    "            depth_df[f'AskPriceL{level}'] = [base_price + 0.01*level + np.random.normal(0, 0.02) for _ in range(len(timestamps))]\n",
    "            depth_df[f'AskQtyL{level}'] = [np.random.exponential(2) for _ in range(len(timestamps))]\n",
    "        \n",
    "        dates[date_str] = depth_df\n",
    "        \n",
    "        # Create synthetic trade data (fewer trades than depth data)\n",
    "        trade_timestamps = sorted(np.random.choice(timestamps, size=int(len(timestamps)*0.05), replace=False))\n",
    "        trade_df = pd.DataFrame(index=trade_timestamps)\n",
    "        trade_df['Price'] = [605.0 + np.random.normal(0, 0.2) for _ in range(len(trade_timestamps))]\n",
    "        trade_df['Quantity'] = [np.random.exponential(1) for _ in range(len(trade_timestamps))]\n",
    "        trade_df['IsMarketMaker'] = [np.random.choice([True, False], p=[0.3, 0.7]) for _ in range(len(trade_timestamps))]\n",
    "        trade_df['NumTrades'] = [np.random.randint(1, 5) for _ in range(len(trade_timestamps))]\n",
    "        trade_df['M'] = True  # Assuming 'M' is some flag always set to True\n",
    "        \n",
    "        trade_dates[date_str] = trade_df\n",
    "    \n",
    "    return dates, trade_dates\n",
    "\n",
    "depth_data, trade_data = create_synthetic_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d48f59c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting basic features from depth data...\n",
      "Adding trade features to the dataset...\n",
      "\n",
      "Feature dataset sample:\n",
      "                     bid_ask_spread   mid_price  imbalance_lvl1  \\\n",
      "2025-03-14 00:00:00        1.291286  604.676983       -0.051213   \n",
      "2025-03-14 00:00:01        1.345523  604.658099        0.762085   \n",
      "2025-03-14 00:00:02        1.307937  604.678843       -0.669070   \n",
      "2025-03-14 00:00:03        1.292335  604.669307       -0.456128   \n",
      "2025-03-14 00:00:04        1.299332  604.673042        0.002897   \n",
      "\n",
      "                     imbalance_lvl2  imbalance_lvl3  imbalance_lvl4  \\\n",
      "2025-03-14 00:00:00        0.719433       -0.700780        0.028288   \n",
      "2025-03-14 00:00:01       -0.421668       -0.376748        0.310946   \n",
      "2025-03-14 00:00:02        0.893577       -0.179744        0.162550   \n",
      "2025-03-14 00:00:03        0.520779       -0.763065        0.471272   \n",
      "2025-03-14 00:00:04       -0.967221        0.304086        0.019762   \n",
      "\n",
      "                     imbalance_lvl5  microprice  cum_bid_qty  cum_ask_qty  \\\n",
      "2025-03-14 00:00:00       -0.144013  604.643918    35.788741    28.499735   \n",
      "2025-03-14 00:00:01       -0.916117  605.170801    51.961402    38.225651   \n",
      "2025-03-14 00:00:02        0.615597  604.241293    40.870140    25.378478   \n",
      "2025-03-14 00:00:03       -0.490262  604.374572    38.287624    28.903515   \n",
      "2025-03-14 00:00:04       -0.432418  604.674923    30.150572    49.985168   \n",
      "\n",
      "                     ...  volume_acceleration_30s  trade_volume_60s  \\\n",
      "2025-03-14 00:00:00  ...                      0.0          1.524365   \n",
      "2025-03-14 00:00:01  ...                      0.0          1.524365   \n",
      "2025-03-14 00:00:02  ...                      0.0          1.524365   \n",
      "2025-03-14 00:00:03  ...                      0.0          1.524365   \n",
      "2025-03-14 00:00:04  ...                      0.0          1.524365   \n",
      "\n",
      "                     buy_volume_60s  sell_volume_60s  volume_imbalance_60s  \\\n",
      "2025-03-14 00:00:00             0.0         1.524365                  -1.0   \n",
      "2025-03-14 00:00:01             0.0         1.524365                  -1.0   \n",
      "2025-03-14 00:00:02             0.0         1.524365                  -1.0   \n",
      "2025-03-14 00:00:03             0.0         1.524365                  -1.0   \n",
      "2025-03-14 00:00:04             0.0         1.524365                  -1.0   \n",
      "\n",
      "                     trade_count_60s    vwap_60s  vwap_shift_60s  \\\n",
      "2025-03-14 00:00:00                1  604.705219             0.0   \n",
      "2025-03-14 00:00:01                1  604.705219             0.0   \n",
      "2025-03-14 00:00:02                1  604.705219             0.0   \n",
      "2025-03-14 00:00:03                1  604.705219             0.0   \n",
      "2025-03-14 00:00:04                1  604.705219             0.0   \n",
      "\n",
      "                     volume_acceleration_60s  mm_participation_ratio  \n",
      "2025-03-14 00:00:00                      0.0                     1.0  \n",
      "2025-03-14 00:00:01                      0.0                     1.0  \n",
      "2025-03-14 00:00:02                      0.0                     1.0  \n",
      "2025-03-14 00:00:03                      0.0                     1.0  \n",
      "2025-03-14 00:00:04                      0.0                     1.0  \n",
      "\n",
      "[5 rows x 56 columns]\n",
      "\n",
      "Total features: 56\n"
     ]
    }
   ],
   "source": [
    "# 2. Feature Engineering\n",
    "\n",
    "def extract_basic_features(depth_df, window_sizes=[10, 30, 60]):\n",
    "    \"\"\"Extract basic features from depth data.\"\"\"\n",
    "    print(\"Extracting basic features from depth data...\")\n",
    "    \n",
    "    features = pd.DataFrame(index=depth_df.index)\n",
    "    \n",
    "    # Liquidity & Depth Features\n",
    "    features['bid_ask_spread'] = depth_df['AskPriceL1'] - depth_df['BidPriceL1']\n",
    "    features['mid_price'] = (depth_df['AskPriceL1'] + depth_df['BidPriceL1']) / 2\n",
    "    \n",
    "    # Order book imbalance at each level\n",
    "    for level in range(1, 6):  # Computing for first 5 levels\n",
    "        bid_qty = depth_df[f'BidQtyL{level}']\n",
    "        ask_qty = depth_df[f'AskQtyL{level}']\n",
    "        features[f'imbalance_lvl{level}'] = (bid_qty - ask_qty) / (bid_qty + ask_qty + 1e-10)\n",
    "    \n",
    "    # Microprice\n",
    "    features['microprice'] = (depth_df['BidPriceL1'] * depth_df['AskQtyL1'] + \n",
    "                             depth_df['AskPriceL1'] * depth_df['BidQtyL1']) / (\n",
    "                             depth_df['BidQtyL1'] + depth_df['AskQtyL1'] + 1e-10)\n",
    "    \n",
    "    # Cumulative depth\n",
    "    cum_bid_qty = sum(depth_df[f'BidQtyL{level}'] for level in range(1, 21))\n",
    "    cum_ask_qty = sum(depth_df[f'AskQtyL{level}'] for level in range(1, 21))\n",
    "    features['cum_bid_qty'] = cum_bid_qty\n",
    "    features['cum_ask_qty'] = cum_ask_qty\n",
    "    features['cum_qty_imbalance'] = (cum_bid_qty - cum_ask_qty) / (cum_bid_qty + cum_ask_qty + 1e-10)\n",
    "    \n",
    "    # Sloped depth calculation\n",
    "    bid_slope = []\n",
    "    ask_slope = []\n",
    "    \n",
    "    for i in range(len(depth_df)):\n",
    "        # Calculate average slope in bid side (how quickly quantity decreases)\n",
    "        bid_prices = [depth_df[f'BidPriceL{level}'].iloc[i] for level in range(1, 21)]\n",
    "        bid_qtys = [depth_df[f'BidQtyL{level}'].iloc[i] for level in range(1, 21)]\n",
    "        bid_vwap = sum(p*q for p, q in zip(bid_prices, bid_qtys)) / (sum(bid_qtys) + 1e-10)\n",
    "        bid_slope.append(abs(bid_vwap - bid_prices[0]))\n",
    "        \n",
    "        # Calculate average slope in ask side\n",
    "        ask_prices = [depth_df[f'AskPriceL{level}'].iloc[i] for level in range(1, 21)]\n",
    "        ask_qtys = [depth_df[f'AskQtyL{level}'].iloc[i] for level in range(1, 21)]\n",
    "        ask_vwap = sum(p*q for p, q in zip(ask_prices, ask_qtys)) / (sum(ask_qtys) + 1e-10)\n",
    "        ask_slope.append(abs(ask_vwap - ask_prices[0]))\n",
    "    \n",
    "    features['bid_slope'] = bid_slope\n",
    "    features['ask_slope'] = ask_slope\n",
    "    \n",
    "    # Volatility & Price Action Features\n",
    "    features['mid_price_log_return'] = np.log(features['mid_price'] / features['mid_price'].shift(1)).fillna(0)\n",
    "    \n",
    "    # Calculate rolling statistics over multiple windows\n",
    "    for window in window_sizes:\n",
    "        features[f'volatility_{window}s'] = features['mid_price_log_return'].rolling(window=window).std().fillna(0)\n",
    "        features[f'trend_{window}s'] = features['mid_price_log_return'].rolling(window=window).mean().fillna(0)\n",
    "        \n",
    "        # Price acceleration (second derivative)\n",
    "        features[f'price_acceleration_{window}s'] = features['mid_price_log_return'].diff().rolling(window=window).mean().fillna(0)\n",
    "        \n",
    "        # Liquidity acceleration \n",
    "        features[f'spread_acceleration_{window}s'] = features['bid_ask_spread'].diff().rolling(window=window).mean().fillna(0)\n",
    "    \n",
    "    # Add cyclical time features\n",
    "    features['hour_sin'] = np.sin(2 * np.pi * features.index.hour / 24)\n",
    "    features['hour_cos'] = np.cos(2 * np.pi * features.index.hour / 24)\n",
    "    features['day_of_week'] = features.index.dayofweek\n",
    "    features['day_sin'] = np.sin(2 * np.pi * features.index.dayofweek / 7)\n",
    "    features['day_cos'] = np.cos(2 * np.pi * features.index.dayofweek / 7)\n",
    "    \n",
    "    return features\n",
    "\n",
    "def add_trade_features(features_df, trade_df, window_sizes=[10, 30, 60]):\n",
    "    \"\"\"Add trade features to existing feature dataframe.\"\"\"\n",
    "    print(\"Adding trade features to the dataset...\")\n",
    "    \n",
    "    # Resample trade data to match feature index\n",
    "    trade_resampled = pd.DataFrame(index=features_df.index)\n",
    "    \n",
    "    # Prepare trade volume data\n",
    "    trade_df['is_buy'] = trade_df['Price'] >= trade_df['Price'].shift(1)\n",
    "    trade_df['buy_volume'] = np.where(trade_df['is_buy'], trade_df['Quantity'], 0)\n",
    "    trade_df['sell_volume'] = np.where(~trade_df['is_buy'], trade_df['Quantity'], 0)\n",
    "    \n",
    "    # Resample trade data to feature index\n",
    "    for window in window_sizes:\n",
    "        window_seconds = f'{window}s'\n",
    "        \n",
    "        # Calculate trade volumes in each window\n",
    "        trade_vol = trade_df['Quantity'].resample(window_seconds).sum().reindex(features_df.index, method='ffill').fillna(0)\n",
    "        buy_vol = trade_df['buy_volume'].resample(window_seconds).sum().reindex(features_df.index, method='ffill').fillna(0)\n",
    "        sell_vol = trade_df['sell_volume'].resample(window_seconds).sum().reindex(features_df.index, method='ffill').fillna(0)\n",
    "        \n",
    "        # Add trade features to main features dataframe\n",
    "        features_df[f'trade_volume_{window}s'] = trade_vol\n",
    "        features_df[f'buy_volume_{window}s'] = buy_vol\n",
    "        features_df[f'sell_volume_{window}s'] = sell_vol\n",
    "        features_df[f'volume_imbalance_{window}s'] = (buy_vol - sell_vol) / (buy_vol + sell_vol + 1e-10)\n",
    "        \n",
    "        # Count number of trades in window\n",
    "        trade_count = trade_df['NumTrades'].resample(window_seconds).count().reindex(features_df.index, method='ffill').fillna(0)\n",
    "        features_df[f'trade_count_{window}s'] = trade_count\n",
    "        \n",
    "        # Calculate VWAP\n",
    "        vwap_num = (trade_df['Price'] * trade_df['Quantity']).resample(window_seconds).sum().reindex(features_df.index, method='ffill').fillna(0)\n",
    "        vwap_den = trade_df['Quantity'].resample(window_seconds).sum().reindex(features_df.index, method='ffill').fillna(0)\n",
    "        features_df[f'vwap_{window}s'] = vwap_num / (vwap_den + 1e-10)\n",
    "        \n",
    "        # Calculate VWAP shift\n",
    "        features_df[f'vwap_shift_{window}s'] = features_df[f'vwap_{window}s'].diff().fillna(0)\n",
    "        \n",
    "        # Volume acceleration (second derivative)\n",
    "        features_df[f'volume_acceleration_{window}s'] = features_df[f'trade_volume_{window}s'].diff().rolling(window).mean().fillna(0)\n",
    "    \n",
    "    # Market maker participation ratio\n",
    "    mm_volume = trade_df[trade_df['IsMarketMaker']]['Quantity'].resample('60s').sum().reindex(features_df.index, method='ffill').fillna(0)\n",
    "    total_volume = trade_df['Quantity'].resample('60s').sum().reindex(features_df.index, method='ffill').fillna(0)\n",
    "    features_df['mm_participation_ratio'] = mm_volume / (total_volume + 1e-10)\n",
    "    \n",
    "    return features_df\n",
    "\n",
    "# Process one day of data as an example\n",
    "date_key = list(depth_data.keys())[0]\n",
    "depth_df = depth_data[date_key]\n",
    "trade_df = trade_data[date_key]\n",
    "\n",
    "# Extract features\n",
    "features = extract_basic_features(depth_df)\n",
    "features = add_trade_features(features, trade_df)\n",
    "\n",
    "# Clean any potential NaN values\n",
    "features.fillna(0, inplace=True)\n",
    "\n",
    "# Display sample of the features\n",
    "print(\"\\nFeature dataset sample:\")\n",
    "print(features.head())\n",
    "print(f\"\\nTotal features: {features.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e4539f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Normalizing features and reducing dimensions...\n",
      "Explained variance by 20 principal components: 77.18%\n",
      "\n",
      "Reduced features sample:\n",
      "                          PC1       PC2       PC3       PC4       PC5  \\\n",
      "2025-03-14 00:00:00 -1.357905  0.258623  2.381864 -0.120988 -0.413091   \n",
      "2025-03-14 00:00:01 -1.361249 -0.840869  2.412753  0.613030 -0.448101   \n",
      "2025-03-14 00:00:02 -1.354613  0.648232  2.400058  0.458326 -0.447854   \n",
      "2025-03-14 00:00:03 -1.363294 -0.335481  2.388326 -0.022886 -0.400603   \n",
      "2025-03-14 00:00:04 -1.348784  0.413845  2.340187 -0.169862 -0.267976   \n",
      "\n",
      "                          PC6        PC7       PC8       PC9      PC10  \\\n",
      "2025-03-14 00:00:00  0.055259  10.675446  0.384726 -0.624912 -0.166244   \n",
      "2025-03-14 00:00:01  1.467882  10.679077 -1.144993 -0.549370  0.038826   \n",
      "2025-03-14 00:00:02  0.003800  10.745238  2.779333 -0.709034 -0.182637   \n",
      "2025-03-14 00:00:03 -0.587467  10.683595  1.448986 -0.631417 -0.068137   \n",
      "2025-03-14 00:00:04 -1.925196  10.541746 -2.408695 -0.508975 -0.082045   \n",
      "\n",
      "                         PC11      PC12      PC13      PC14      PC15  \\\n",
      "2025-03-14 00:00:00 -0.173080 -1.015047 -0.900822  0.018138  3.021101   \n",
      "2025-03-14 00:00:01 -0.136613  1.974044 -1.092923  2.083922  2.564834   \n",
      "2025-03-14 00:00:02 -0.231867 -1.682865 -0.826629 -0.219077  3.019479   \n",
      "2025-03-14 00:00:03 -0.205879  0.327810 -0.955125  1.148095  3.091121   \n",
      "2025-03-14 00:00:04  0.071020  0.120490 -1.150566 -1.999211  2.438423   \n",
      "\n",
      "                         PC16      PC17      PC18      PC19      PC20  \n",
      "2025-03-14 00:00:00 -1.401606 -1.249876 -0.532020  0.803827 -1.145546  \n",
      "2025-03-14 00:00:01 -0.060117 -0.334450  0.663890  2.205948 -1.051100  \n",
      "2025-03-14 00:00:02 -1.107632 -0.755231 -0.344557  0.782407 -1.128833  \n",
      "2025-03-14 00:00:03 -1.384197 -1.386739 -0.422269  1.061344 -1.061777  \n",
      "2025-03-14 00:00:04  0.309604  1.067171  0.635558  1.125958 -1.033251  \n"
     ]
    }
   ],
   "source": [
    "# 3. Data Normalization and Dimensionality Reduction\n",
    "\n",
    "def normalize_and_reduce_dimensions(features_df, n_components=20):\n",
    "    \"\"\"Normalize features and reduce dimensionality using PCA.\"\"\"\n",
    "    print(\"\\nNormalizing features and reducing dimensions...\")\n",
    "    \n",
    "    # Remove non-numeric columns\n",
    "    numeric_features = features_df.select_dtypes(include=[np.number])\n",
    "    \n",
    "    # Normalize using StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    features_scaled = scaler.fit_transform(numeric_features)\n",
    "    \n",
    "    # Apply PCA for dimensionality reduction\n",
    "    pca = PCA(n_components=n_components)\n",
    "    features_reduced = pca.fit_transform(features_scaled)\n",
    "    \n",
    "    # Create dataframe with reduced dimensions\n",
    "    reduced_df = pd.DataFrame(features_reduced, \n",
    "                              index=numeric_features.index,\n",
    "                              columns=[f'PC{i+1}' for i in range(n_components)])\n",
    "    \n",
    "    # Display explained variance ratio\n",
    "    explained_variance = pca.explained_variance_ratio_\n",
    "    cumulative_variance = np.cumsum(explained_variance)\n",
    "    \n",
    "    print(f\"Explained variance by {n_components} principal components: {cumulative_variance[-1]:.2%}\")\n",
    "    \n",
    "    # Feature importance\n",
    "    feature_importance = pd.DataFrame(\n",
    "        pca.components_.T,\n",
    "        columns=[f'PC{i+1}' for i in range(n_components)],\n",
    "        index=numeric_features.columns\n",
    "    )\n",
    "    \n",
    "    return reduced_df, scaler, pca, feature_importance\n",
    "\n",
    "# Apply normalization and dimensionality reduction\n",
    "reduced_features, scaler, pca, feature_importance = normalize_and_reduce_dimensions(features)\n",
    "\n",
    "# Display reduced features\n",
    "print(\"\\nReduced features sample:\")\n",
    "print(reduced_features.head())\n",
    "\n",
    "# Plot explained variance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(1, len(pca.explained_variance_ratio_) + 1), pca.explained_variance_ratio_)\n",
    "plt.plot(range(1, len(pca.explained_variance_ratio_) + 1), np.cumsum(pca.explained_variance_ratio_), 'r-')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.title('Explained Variance by Principal Components')\n",
    "plt.grid(True)\n",
    "plt.savefig('pca_explained_variance.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77feda6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Ensemble Clustering\n",
    "\n",
    "def apply_kmeans(data, k_range=range(2, 11)):\n",
    "    \"\"\"Apply K-means clustering with various K values and evaluate.\"\"\"\n",
    "    print(\"\\nApplying K-means clustering...\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for k in k_range:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "        labels = kmeans.fit_predict(data)\n",
    "        \n",
    "        silhouette = silhouette_score(data, labels) if len(set(labels)) > 1 else -1\n",
    "        db_index = davies_bouldin_score(data, labels) if len(set(labels)) > 1 else float('inf')\n",
    "        \n",
    "        results.append({\n",
    "            'k': k,\n",
    "            'silhouette': silhouette,\n",
    "            'davies_bouldin': db_index,\n",
    "            'labels': labels,\n",
    "            'model': kmeans\n",
    "        })\n",
    "    \n",
    "    # Find best model based on silhouette score\n",
    "    best_result = max(results, key=lambda x: x['silhouette'])\n",
    "    \n",
    "    print(f\"Best K-means model: k={best_result['k']}, silhouette={best_result['silhouette']:.4f}\")\n",
    "    \n",
    "    return results, best_result\n",
    "\n",
    "def apply_gmm(data, k_range=range(2, 11)):\n",
    "    \"\"\"Apply Gaussian Mixture Model clustering with various K values and evaluate.\"\"\"\n",
    "    print(\"\\nApplying Gaussian Mixture Model clustering...\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for k in k_range:\n",
    "        gmm = GaussianMixture(n_components=k, random_state=42, n_init=10)\n",
    "        gmm.fit(data)\n",
    "        labels = gmm.predict(data)\n",
    "        \n",
    "        silhouette = silhouette_score(data, labels) if len(set(labels)) > 1 else -1\n",
    "        db_index = davies_bouldin_score(data, labels) if len(set(labels)) > 1 else float('inf')\n",
    "        \n",
    "        results.append({\n",
    "            'k': k,\n",
    "            'silhouette': silhouette,\n",
    "            'davies_bouldin': db_index,\n",
    "            'labels': labels,\n",
    "            'model': gmm\n",
    "        })\n",
    "    \n",
    "    # Find best model based on silhouette score\n",
    "    best_result = max(results, key=lambda x: x['silhouette'])\n",
    "    \n",
    "    print(f\"Best GMM model: k={best_result['k']}, silhouette={best_result['silhouette']:.4f}\")\n",
    "    \n",
    "    return results, best_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "128e7653",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_hdbscan(data, min_samples_range=[5, 10, 15, 20, 30], min_cluster_size_range=[10, 20, 30, 50, 100]):\n",
    "    \"\"\"Apply HDBSCAN clustering with various parameter values and evaluate.\"\"\"\n",
    "    print(\"\\nApplying HDBSCAN clustering...\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for min_samples in min_samples_range:\n",
    "        for min_cluster_size in min_cluster_size_range:\n",
    "            try:\n",
    "                hdbscan = HDBSCAN(min_samples=min_samples, min_cluster_size=min_cluster_size)\n",
    "                labels = hdbscan.fit_predict(data)\n",
    "                \n",
    "                # Skip if all points are classified as noise (-1)\n",
    "                if len(set(labels)) <= 1 or all(l == -1 for l in labels):\n",
    "                    continue\n",
    "                \n",
    "                # Replace noise points (-1) temporarily for metric calculation\n",
    "                temp_labels = np.array(labels)\n",
    "                if -1 in temp_labels:\n",
    "                    # Assign noise points to nearest cluster for metric calculation\n",
    "                    noise_indices = np.where(temp_labels == -1)[0]\n",
    "                    non_noise_indices = np.where(temp_labels != -1)[0]\n",
    "                    \n",
    "                    if len(non_noise_indices) > 0:\n",
    "                        from sklearn.neighbors import NearestNeighbors\n",
    "                        nn = NearestNeighbors(n_neighbors=1)\n",
    "                        nn.fit(data[non_noise_indices])\n",
    "                        _, indices = nn.kneighbors(data[noise_indices])\n",
    "                        \n",
    "                        for i, idx in enumerate(noise_indices):\n",
    "                            temp_labels[idx] = temp_labels[non_noise_indices[indices[i][0]]]\n",
    "                \n",
    "                # Calculate metrics if we have more than one cluster\n",
    "                if len(set(temp_labels)) > 1:\n",
    "                    silhouette = silhouette_score(data, temp_labels)\n",
    "                    db_index = davies_bouldin_score(data, temp_labels)\n",
    "                else:\n",
    "                    silhouette = -1\n",
    "                    db_index = float('inf')\n",
    "                \n",
    "                results.append({\n",
    "                    'min_samples': min_samples,\n",
    "                    'min_cluster_size': min_cluster_size,\n",
    "                    'silhouette': silhouette,\n",
    "                    'davies_bouldin': db_index,\n",
    "                    'labels': labels,\n",
    "                    'model': hdbscan\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Error with HDBSCAN parameters min_samples={min_samples}, min_cluster_size={min_cluster_size}: {e}\")\n",
    "    \n",
    "    if not results:\n",
    "        print(\"No valid HDBSCAN results found. Using default parameters.\")\n",
    "        hdbscan = HDBSCAN(min_samples=5, min_cluster_size=20)\n",
    "        labels = hdbscan.fit_predict(data)\n",
    "        results.append({\n",
    "            'min_samples': 5,\n",
    "            'min_cluster_size': 20,\n",
    "            'silhouette': -1,\n",
    "            'davies_bouldin': float('inf'),\n",
    "            'labels': labels,\n",
    "            'model': hdbscan\n",
    "        })\n",
    "    \n",
    "    # Find best model based on silhouette score\n",
    "    best_result = max(results, key=lambda x: x['silhouette'])\n",
    "    \n",
    "    print(f\"Best HDBSCAN model: min_samples={best_result['min_samples']}, min_cluster_size={best_result['min_cluster_size']}, silhouette={best_result['silhouette']:.4f}\")\n",
    "    \n",
    "    return results, best_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "932ff642",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ensemble_clustering(kmeans_labels, gmm_labels, hdbscan_labels, n_clusters=None):\n",
    "    \"\"\"Create ensemble clustering by combining results from multiple algorithms.\"\"\"\n",
    "    print(\"\\nCreating ensemble clustering...\")\n",
    "    \n",
    "    # Stack labels from different algorithms\n",
    "    combined_data = np.column_stack([kmeans_labels, gmm_labels, hdbscan_labels])\n",
    "    \n",
    "    # Apply K-means on the combined labels to get consensus clusters\n",
    "    if n_clusters is None:\n",
    "        n_clusters = len(set(kmeans_labels))  # Use number of clusters from K-means as default\n",
    "    \n",
    "    consensus_kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "    consensus_labels = consensus_kmeans.fit_predict(combined_data)\n",
    "    \n",
    "    # Evaluate ensemble results\n",
    "    silhouette = silhouette_score(reduced_features, consensus_labels)\n",
    "    db_index = davies_bouldin_score(reduced_features, consensus_labels)\n",
    "    \n",
    "    print(f\"Ensemble clustering: n_clusters={n_clusters}, silhouette={silhouette:.4f}, davies_bouldin={db_index:.4f}\")\n",
    "    \n",
    "    return consensus_labels, consensus_kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ff708642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Applying K-means clustering...\n",
      "Best K-means model: k=2, silhouette=0.1424\n",
      "\n",
      "Applying Gaussian Mixture Model clustering...\n",
      "Best GMM model: k=2, silhouette=0.0729\n",
      "\n",
      "Applying HDBSCAN clustering...\n",
      "Error with HDBSCAN parameters min_samples=5, min_cluster_size=10: \"None of [Index([    0,     1,     2,     3,     4,     5,     6,     7,     8,     9,\\n       ...\\n       86390, 86391, 86392, 86393, 86394, 86395, 86396, 86397, 86398, 86399],\\n      dtype='int64', length=86321)] are in the [columns]\"\n",
      "Error with HDBSCAN parameters min_samples=5, min_cluster_size=100: \"None of [Index([    0,     1,     2,     3,     4,     5,     6,     7,     8,     9,\\n       ...\\n       86390, 86391, 86392, 86393, 86394, 86395, 86396, 86397, 86398, 86399],\\n      dtype='int64', length=86156)] are in the [columns]\"\n",
      "Error with HDBSCAN parameters min_samples=10, min_cluster_size=100: \"None of [Index([    0,     1,     2,     3,     4,     5,     6,     7,     8,     9,\\n       ...\\n       86390, 86391, 86392, 86393, 86394, 86395, 86396, 86397, 86398, 86399],\\n      dtype='int64', length=86052)] are in the [columns]\"\n",
      "Error with HDBSCAN parameters min_samples=15, min_cluster_size=100: \"None of [Index([    0,     1,     2,     3,     4,     5,     6,     7,     9,    11,\\n       ...\\n       86390, 86391, 86392, 86393, 86394, 86395, 86396, 86397, 86398, 86399],\\n      dtype='int64', length=85920)] are in the [columns]\"\n",
      "Error with HDBSCAN parameters min_samples=20, min_cluster_size=100: \"None of [Index([    0,     3,     4,     5,     7,     9,    11,    13,    15,    19,\\n       ...\\n       86390, 86391, 86392, 86393, 86394, 86395, 86396, 86397, 86398, 86399],\\n      dtype='int64', length=85806)] are in the [columns]\"\n",
      "Error with HDBSCAN parameters min_samples=30, min_cluster_size=50: \"None of [Index([   21,    22,    23,    25,    26,    27,    28,    29,    31,    32,\\n       ...\\n       86390, 86391, 86392, 86393, 86394, 86395, 86396, 86397, 86398, 86399],\\n      dtype='int64', length=85662)] are in the [columns]\"\n",
      "Error with HDBSCAN parameters min_samples=30, min_cluster_size=100: \"None of [Index([   21,    22,    23,    25,    26,    27,    28,    29,    31,    32,\\n       ...\\n       86390, 86391, 86392, 86393, 86394, 86395, 86396, 86397, 86398, 86399],\\n      dtype='int64', length=85536)] are in the [columns]\"\n",
      "Best HDBSCAN model: min_samples=5, min_cluster_size=20, silhouette=0.6810\n"
     ]
    }
   ],
   "source": [
    "# Apply different clustering algorithms\n",
    "kmeans_results, best_kmeans = apply_kmeans(reduced_features)\n",
    "gmm_results, best_gmm = apply_gmm(reduced_features)\n",
    "hdbscan_results, best_hdbscan = apply_hdbscan(reduced_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "373ded42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating ensemble clustering...\n",
      "Ensemble clustering: n_clusters=2, silhouette=0.1424, davies_bouldin=2.6031\n"
     ]
    }
   ],
   "source": [
    "# Create ensemble clustering\n",
    "ensemble_labels, ensemble_model = create_ensemble_clustering(\n",
    "    best_kmeans['labels'], \n",
    "    best_gmm['labels'], \n",
    "    best_hdbscan['labels']\n",
    ")\n",
    "\n",
    "# Plot silhouette scores for different K values\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot([r['k'] for r in kmeans_results], [r['silhouette'] for r in kmeans_results], 'o-', label='K-means')\n",
    "plt.plot([r['k'] for r in gmm_results], [r['silhouette'] for r in gmm_results], 's-', label='GMM')\n",
    "plt.xlabel('Number of Clusters (K)')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Scores for Different Clustering Methods')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig('silhouette_scores.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e57ab098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing cluster characteristics...\n",
      "\n",
      "Regime Characteristics:\n",
      "Regime 0 (Trending Down & Liquid & Stable):\n",
      "  - Trend: Trending Down\n",
      "  - Volatility: Stable\n",
      "  - Liquidity: Liquid\n",
      "  - Order Book Pressure: Balanced\n",
      "\n",
      "Regime 1 (Mean-Reverting & Liquid & Stable):\n",
      "  - Trend: Mean-Reverting\n",
      "  - Volatility: Stable\n",
      "  - Liquidity: Liquid\n",
      "  - Order Book Pressure: Balanced\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5. Regime Labeling and Analysis\n",
    "\n",
    "def analyze_clusters(features_df, reduced_df, labels, analysis_features=None):\n",
    "    \"\"\"Analyze characteristics of each cluster/regime.\"\"\"\n",
    "    print(\"\\nAnalyzing cluster characteristics...\")\n",
    "    \n",
    "    if analysis_features is None:\n",
    "        # Select most important features for analysis\n",
    "        analysis_features = [\n",
    "            'bid_ask_spread', 'mid_price_log_return', 'volatility_30s', 'trend_30s',\n",
    "            'cum_qty_imbalance', 'volume_imbalance_30s', 'trade_volume_30s',\n",
    "            'price_acceleration_30s', 'volume_acceleration_30s'\n",
    "        ]\n",
    "    \n",
    "    # Add labels to original features dataframe\n",
    "    labeled_df = features_df.copy()\n",
    "    labeled_df['cluster'] = labels\n",
    "    \n",
    "    # Calculate statistics for each cluster\n",
    "    cluster_stats = labeled_df.groupby('cluster')[analysis_features].mean()\n",
    "    \n",
    "    # Calculate standard deviations for each cluster\n",
    "    cluster_stds = labeled_df.groupby('cluster')[analysis_features].std()\n",
    "    \n",
    "    # Determine regime characteristics\n",
    "    regime_characteristics = {}\n",
    "    \n",
    "    for cluster in cluster_stats.index:\n",
    "        stats = cluster_stats.loc[cluster]\n",
    "        \n",
    "        # Trending vs Mean-reverting\n",
    "        trending_score = stats['trend_30s'] / (cluster_stats['trend_30s'].std() + 1e-10)\n",
    "        regime_trend = \"Trending\" if abs(trending_score) > 0.5 else \"Mean-Reverting\"\n",
    "        if trending_score > 0.5:\n",
    "            regime_trend = \"Trending Up\"\n",
    "        elif trending_score < -0.5:\n",
    "            regime_trend = \"Trending Down\"\n",
    "        \n",
    "        # Volatile vs Stable\n",
    "        volatility_score = stats['volatility_30s'] / (cluster_stats['volatility_30s'].mean() + 1e-10)\n",
    "        regime_volatility = \"Volatile\" if volatility_score > 1.2 else \"Stable\"\n",
    "        \n",
    "        # Liquid vs Illiquid\n",
    "        liquidity_score = stats['bid_ask_spread'] / (cluster_stats['bid_ask_spread'].mean() + 1e-10)\n",
    "        regime_liquidity = \"Illiquid\" if liquidity_score > 1.2 else \"Liquid\"\n",
    "        \n",
    "        # Create regime name\n",
    "        regime_name = f\"{regime_trend} & {regime_liquidity} & {regime_volatility}\"\n",
    "        \n",
    "        # Additional insights\n",
    "        imbalance = \"Buy Pressure\" if stats['cum_qty_imbalance'] > 0.1 else \"Sell Pressure\" if stats['cum_qty_imbalance'] < -0.1 else \"Balanced\"\n",
    "        \n",
    "        regime_characteristics[cluster] = {\n",
    "            'name': regime_name,\n",
    "            'trend': regime_trend,\n",
    "            'volatility': regime_volatility,\n",
    "            'liquidity': regime_liquidity,\n",
    "            'imbalance': imbalance,\n",
    "            'stats': stats\n",
    "        }\n",
    "    \n",
    "    return regime_characteristics, labeled_df\n",
    "\n",
    "# Analyze ensemble clustering results\n",
    "regime_characteristics, labeled_df = analyze_clusters(features, reduced_features, ensemble_labels)\n",
    "\n",
    "# Display regime characteristics\n",
    "print(\"\\nRegime Characteristics:\")\n",
    "for cluster, details in regime_characteristics.items():\n",
    "    print(f\"Regime {cluster} ({details['name']}):\")\n",
    "    print(f\"  - Trend: {details['trend']}\")\n",
    "    print(f\"  - Volatility: {details['volatility']}\")\n",
    "    print(f\"  - Liquidity: {details['liquidity']}\")\n",
    "    print(f\"  - Order Book Pressure: {details['imbalance']}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "92bad937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Market Impact Analysis\n",
    "\n",
    "def analyze_market_impact(labeled_df, trade_df):\n",
    "    \"\"\"Analyze how trades impact the market in different regimes.\"\"\"\n",
    "    print(\"\\nAnalyzing market impact in different regimes...\")\n",
    "    \n",
    "    # Create a dataframe to store market impact results\n",
    "    impact_results = pd.DataFrame(columns=['regime', 'trade_size_quantile', 'price_impact', 'recovery_time'])\n",
    "    \n",
    "    # Merge trade data with regime labels\n",
    "    labeled_trades = pd.DataFrame(index=trade_df.index)\n",
    "    labeled_trades['Price'] = trade_df['Price']\n",
    "    labeled_trades['Quantity'] = trade_df['Quantity']\n",
    "    \n",
    "    # Assign regime to each trade (nearest in time)\n",
    "    labeled_trade_indices = labeled_trades.index\n",
    "    nearest_indices = []\n",
    "    \n",
    "    for trade_time in labeled_trade_indices:\n",
    "        # Find the nearest timestamp in labeled_df\n",
    "        nearest_idx = labeled_df.index[labeled_df.index.get_indexer([trade_time], method='nearest')[0]]\n",
    "        nearest_indices.append(nearest_idx)\n",
    "    \n",
    "    labeled_trades['regime'] = [labeled_df.loc[idx, 'cluster'] for idx in nearest_indices]\n",
    "    \n",
    "    # Analyze price impact for different trade size quantiles in each regime\n",
    "    regimes = labeled_trades['regime'].unique()\n",
    "    quantiles = [0.25, 0.5, 0.75, 0.9]  # Small, medium, large, very large trades\n",
    "\n",
    "    for regime in regimes:\n",
    "        regime_trades = labeled_trades[labeled_trades['regime'] == regime]\n",
    "        \n",
    "        # Skip if not enough trades\n",
    "        if len(regime_trades) < 20:\n",
    "            continue\n",
    "        \n",
    "        # Calculate trade size quantiles\n",
    "        size_quantiles = regime_trades['Quantity'].quantile(quantiles)\n",
    "        \n",
    "        for i, q in enumerate(quantiles):\n",
    "            quantile_name = f\"Q{int(q*100)}\"\n",
    "            quantile_value = size_quantiles[q]\n",
    "            \n",
    "            # Filter trades in this size quantile\n",
    "            if i < len(quantiles) - 1:\n",
    "                size_mask = (regime_trades['Quantity'] >= quantile_value) & (regime_trades['Quantity'] < size_quantiles[quantiles[i+1]])\n",
    "            else:\n",
    "                size_mask = (regime_trades['Quantity'] >= quantile_value)\n",
    "            \n",
    "            quantile_trades = regime_trades[size_mask]\n",
    "            \n",
    "            # Skip if not enough trades in this quantile\n",
    "            if len(quantile_trades) < 10:\n",
    "                continue\n",
    "            \n",
    "            # Calculate average price impact (5 seconds after trade)\n",
    "            avg_impact = 0\n",
    "            avg_recovery = 0\n",
    "            \n",
    "            for idx, trade in quantile_trades.iterrows():\n",
    "                # Find price before and after trade\n",
    "                try:\n",
    "                    # Price before trade\n",
    "                    pre_trade_time = idx - timedelta(seconds=1)\n",
    "                    pre_trade_price = labeled_df.loc[labeled_df.index.get_indexer([pre_trade_time], method='nearest')[0], 'mid_price']\n",
    "                    \n",
    "                    # Price immediately after trade (impact)\n",
    "                    post_trade_time = idx + timedelta(seconds=5)\n",
    "                    post_idx = labeled_df.index.get_indexer([post_trade_time], method='nearest')[0]\n",
    "                    post_trade_price = labeled_df.iloc[post_idx]['mid_price']\n",
    "                    \n",
    "                    # Price recovery (30 seconds after trade)\n",
    "                    recovery_time = idx + timedelta(seconds=30)\n",
    "                    recovery_idx = labeled_df.index.get_indexer([recovery_time], method='nearest')[0]\n",
    "                    recovery_price = labeled_df.iloc[recovery_idx]['mid_price']\n",
    "                    \n",
    "                    # Calculate impact and recovery\n",
    "                    impact = (post_trade_price - pre_trade_price) / pre_trade_price\n",
    "                    recovery = 1 - abs((recovery_price - pre_trade_price) / (post_trade_price - pre_trade_price + 1e-10))\n",
    "                    \n",
    "                    avg_impact += impact\n",
    "                    avg_recovery += recovery\n",
    "                except:\n",
    "                    # Skip if can't calculate impact\n",
    "                    continue\n",
    "            \n",
    "            if len(quantile_trades) > 0:\n",
    "                avg_impact /= len(quantile_trades)\n",
    "                avg_recovery /= len(quantile_trades)\n",
    "                \n",
    "                # Add to results\n",
    "                impact_results = impact_results.append({\n",
    "                    'regime': regime_characteristics[regime]['name'],\n",
    "                    'trade_size_quantile': quantile_name,\n",
    "                    'price_impact': avg_impact * 10000,  # Convert to basis points\n",
    "                    'recovery_time': avg_recovery\n",
    "                }, ignore_index=True)\n",
    "    \n",
    "    return impact_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "69dab611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run market impact analysis\n",
    "# impact_results = analyze_market_impact(labeled_df, trade_data[list(trade_data.keys())[0]])\n",
    "# With simulated data we'll create a synthetic impact analysis\n",
    "def create_synthetic_impact_analysis():\n",
    "    \"\"\"Create synthetic market impact analysis results for demonstration.\"\"\"\n",
    "    impact_results = []\n",
    "    \n",
    "    for regime_id, regime_info in regime_characteristics.items():\n",
    "        regime_name = regime_info['name']\n",
    "        \n",
    "        # Generate synthetic impact data based on regime characteristics\n",
    "        for size_quantile in ['Q25', 'Q50', 'Q75', 'Q90']:\n",
    "            # Base impact depends on volatility and liquidity\n",
    "            base_impact = 2.0  # basis points\n",
    "            \n",
    "            if 'Volatile' in regime_name:\n",
    "                base_impact *= 1.5\n",
    "            if 'Illiquid' in regime_name:\n",
    "                base_impact *= 2.0\n",
    "            if 'Trending' in regime_name:\n",
    "                base_impact *= 1.2\n",
    "                \n",
    "            # Size multiplier\n",
    "            size_multiplier = {'Q25': 0.5, 'Q50': 1.0, 'Q75': 2.0, 'Q90': 3.5}\n",
    "            \n",
    "            # Calculate impact and recovery\n",
    "            impact = base_impact * size_multiplier[size_quantile]\n",
    "            recovery = 0.7\n",
    "            \n",
    "            if 'Volatile' in regime_name:\n",
    "                recovery *= 0.8\n",
    "            if 'Illiquid' in regime_name:\n",
    "                recovery *= 0.7\n",
    "                \n",
    "            impact_results.append({\n",
    "                'regime': regime_name,\n",
    "                'trade_size_quantile': size_quantile,\n",
    "                'price_impact': impact,\n",
    "                'recovery_time': recovery\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(impact_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8f6ca075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Market Impact Analysis by Regime and Trade Size:\n",
      "                             regime trade_size_quantile  price_impact  \\\n",
      "0   Trending Down & Liquid & Stable                 Q25           1.2   \n",
      "1   Trending Down & Liquid & Stable                 Q50           2.4   \n",
      "2   Trending Down & Liquid & Stable                 Q75           4.8   \n",
      "3   Trending Down & Liquid & Stable                 Q90           8.4   \n",
      "4  Mean-Reverting & Liquid & Stable                 Q25           1.0   \n",
      "5  Mean-Reverting & Liquid & Stable                 Q50           2.0   \n",
      "6  Mean-Reverting & Liquid & Stable                 Q75           4.0   \n",
      "7  Mean-Reverting & Liquid & Stable                 Q90           7.0   \n",
      "\n",
      "   recovery_time  \n",
      "0            0.7  \n",
      "1            0.7  \n",
      "2            0.7  \n",
      "3            0.7  \n",
      "4            0.7  \n",
      "5            0.7  \n",
      "6            0.7  \n",
      "7            0.7  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1400x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create synthetic impact results\n",
    "impact_results = create_synthetic_impact_analysis()\n",
    "\n",
    "# Display market impact results\n",
    "print(\"\\nMarket Impact Analysis by Regime and Trade Size:\")\n",
    "print(impact_results)\n",
    "\n",
    "# Plot market impact by regime and trade size\n",
    "plt.figure(figsize=(14, 8))\n",
    "regimes = impact_results['regime'].unique()\n",
    "quantiles = ['Q25', 'Q50', 'Q75', 'Q90']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fa1b3c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "bar_width = 0.2\n",
    "index = np.arange(len(regimes))\n",
    "\n",
    "for i, q in enumerate(quantiles):\n",
    "    values = [impact_results[(impact_results['regime'] == r) & (impact_results['trade_size_quantile'] == q)]['price_impact'].values[0] \n",
    "              if not impact_results[(impact_results['regime'] == r) & (impact_results['trade_size_quantile'] == q)].empty else 0\n",
    "              for r in regimes]\n",
    "    plt.bar(index + i*bar_width, values, bar_width, label=q)\n",
    "\n",
    "plt.xlabel('Market Regime')\n",
    "plt.ylabel('Price Impact (Basis Points)')\n",
    "plt.title('Market Impact by Regime and Trade Size')\n",
    "plt.xticks(index + bar_width * 1.5, [r.split('&')[0][:10] + '...' for r in regimes], rotation=45)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('market_impact.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9821effa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing regime transitions...\n",
      "\n",
      "Regime Transition Probabilities:\n",
      "                                  Trending Down & Liquid & Stable  \\\n",
      "Trending Down & Liquid & Stable                               0.0   \n",
      "Mean-Reverting & Liquid & Stable                              1.0   \n",
      "\n",
      "                                  Mean-Reverting & Liquid & Stable  \n",
      "Trending Down & Liquid & Stable                                1.0  \n",
      "Mean-Reverting & Liquid & Stable                               0.0  \n"
     ]
    }
   ],
   "source": [
    "# 7. Regime Transition Analysis\n",
    "\n",
    "def analyze_regime_transitions(labeled_df):\n",
    "    \"\"\"Analyze transitions between different market regimes.\"\"\"\n",
    "    print(\"\\nAnalyzing regime transitions...\")\n",
    "    \n",
    "    # Create transition matrix\n",
    "    regime_labels = sorted(labeled_df['cluster'].unique())\n",
    "    n_regimes = len(regime_labels)\n",
    "    transitions = np.zeros((n_regimes, n_regimes))\n",
    "    \n",
    "    # Count transitions\n",
    "    for i in range(1, len(labeled_df)):\n",
    "        prev_regime = labeled_df['cluster'].iloc[i-1]\n",
    "        curr_regime = labeled_df['cluster'].iloc[i]\n",
    "        \n",
    "        if prev_regime != curr_regime:\n",
    "            prev_idx = list(regime_labels).index(prev_regime)\n",
    "            curr_idx = list(regime_labels).index(curr_regime)\n",
    "            transitions[prev_idx, curr_idx] += 1\n",
    "    \n",
    "    # Convert to probabilities\n",
    "    transition_prob = np.zeros_like(transitions, dtype=float)\n",
    "    for i in range(n_regimes):\n",
    "        row_sum = transitions[i].sum()\n",
    "        if row_sum > 0:\n",
    "            transition_prob[i] = transitions[i] / row_sum\n",
    "    \n",
    "    # Create transition dataframe\n",
    "    transition_df = pd.DataFrame(\n",
    "        transition_prob,\n",
    "        index=[regime_characteristics[label]['name'] for label in regime_labels],\n",
    "        columns=[regime_characteristics[label]['name'] for label in regime_labels]\n",
    "    )\n",
    "    \n",
    "    return transition_df\n",
    "\n",
    "# Run regime transition analysis\n",
    "transition_df = analyze_regime_transitions(labeled_df)\n",
    "\n",
    "# Display transition probabilities\n",
    "print(\"\\nRegime Transition Probabilities:\")\n",
    "print(transition_df)\n",
    "\n",
    "# Plot transition heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(transition_df, annot=True, cmap='Blues', fmt='.2f', linewidths=.5)\n",
    "plt.xlabel('To Regime')\n",
    "plt.ylabel('From Regime')\n",
    "plt.title('Regime Transition Probabilities')\n",
    "plt.tight_layout()\n",
    "plt.savefig('regime_transitions.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1ca8e15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_regimes_over_time(labeled_df):\n",
    "    \"\"\"Visualize regime evolution over time with price overlay.\"\"\"\n",
    "    print(\"\\nCreating regime visualization over time...\")\n",
    "    \n",
    "    # Create figure for visualization\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(20, 12), sharex=True, gridspec_kw={'height_ratios': [3, 1]})\n",
    "    \n",
    "    # Plot mid-price\n",
    "    ax1.plot(labeled_df.index, labeled_df['mid_price'], color='black', alpha=0.7)\n",
    "    ax1.set_ylabel('Mid Price')\n",
    "    ax1.set_title('Market Price and Regime Evolution')\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # Plot regimes as background color\n",
    "    unique_regimes = sorted(labeled_df['cluster'].unique())\n",
    "    cmap = plt.cm.get_cmap('tab10', len(unique_regimes))\n",
    "    colors = {regime: cmap(i) for i, regime in enumerate(unique_regimes)}\n",
    "    \n",
    "    # Get regime change points\n",
    "    labeled_df['regime_change'] = labeled_df['cluster'] != labeled_df['cluster'].shift(1)\n",
    "    change_points = labeled_df[labeled_df['regime_change']].index.tolist()\n",
    "    change_points = [labeled_df.index[0]] + change_points + [labeled_df.index[-1]]\n",
    "    \n",
    "    # Plot regimes as background\n",
    "    for i in range(len(change_points) - 1):\n",
    "        start = change_points[i]\n",
    "        end = change_points[i+1]\n",
    "        regime = labeled_df.loc[start:end, 'cluster'].iloc[0]\n",
    "        ax1.axvspan(start, end, alpha=0.3, color=colors[regime])\n",
    "    \n",
    "    # Plot regime labels\n",
    "    ax2.scatter(labeled_df.index, labeled_df['cluster'], c=labeled_df['cluster'], cmap=cmap, s=5)\n",
    "    ax2.set_yticks(unique_regimes)\n",
    "    ax2.set_yticklabels([regime_characteristics[r]['name'] for r in unique_regimes], fontsize=10)\n",
    "    ax2.set_ylabel('Market Regime')\n",
    "    ax2.set_xlabel('Time')\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    # Format x-axis\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('regime_evolution.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7155bcfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating features for regime duration prediction...\n",
      "\n",
      "Training regime duration prediction model...\n",
      "\n",
      "Feature Importance for Duration Prediction:\n",
      "                       feature  importance\n",
      "11        start_bid_ask_spread    0.193007\n",
      "9         start_volatility_30s    0.149253\n",
      "1                       minute    0.137262\n",
      "10             start_trend_30s    0.113223\n",
      "12  start_volume_imbalance_30s    0.110291\n",
      "3                     hour_sin    0.064316\n",
      "7                     regime_1    0.061940\n",
      "4                     hour_cos    0.058747\n",
      "8                     regime_0    0.056473\n",
      "0                         hour    0.055488\n"
     ]
    }
   ],
   "source": [
    "# 8. Regime Duration Forecasting\n",
    "\n",
    "def create_duration_features(labeled_df):\n",
    "    \"\"\"Create features for regime duration prediction.\"\"\"\n",
    "    print(\"\\nCreating features for regime duration prediction...\")\n",
    "    \n",
    "    # Identify regime change points\n",
    "    labeled_df['regime_change'] = labeled_df['cluster'] != labeled_df['cluster'].shift(1)\n",
    "    regime_changes = labeled_df[labeled_df['regime_change']].index\n",
    "    \n",
    "    # Calculate regime durations\n",
    "    durations = []\n",
    "    regimes = []\n",
    "    start_times = []\n",
    "    \n",
    "    for i in range(len(regime_changes) - 1):\n",
    "        start = regime_changes[i]\n",
    "        end = regime_changes[i+1]\n",
    "        regime = labeled_df.loc[start, 'cluster']\n",
    "        duration = (end - start).total_seconds()\n",
    "        \n",
    "        # Skip very short regimes (likely noise)\n",
    "        if duration < 10:\n",
    "            continue\n",
    "        \n",
    "        durations.append(duration)\n",
    "        regimes.append(regime)\n",
    "        start_times.append(start)\n",
    "    \n",
    "    # Create duration dataframe\n",
    "    if durations:\n",
    "        duration_df = pd.DataFrame({\n",
    "            'start_time': start_times,\n",
    "            'regime': regimes,\n",
    "            'duration': durations\n",
    "        })\n",
    "        \n",
    "        # Add time features\n",
    "        duration_df['hour'] = duration_df['start_time'].dt.hour\n",
    "        duration_df['minute'] = duration_df['start_time'].dt.minute\n",
    "        duration_df['day_of_week'] = duration_df['start_time'].dt.dayofweek\n",
    "        \n",
    "        # Add cyclical time features\n",
    "        duration_df['hour_sin'] = np.sin(2 * np.pi * duration_df['hour'] / 24)\n",
    "        duration_df['hour_cos'] = np.cos(2 * np.pi * duration_df['hour'] / 24)\n",
    "        duration_df['day_sin'] = np.sin(2 * np.pi * duration_df['day_of_week'] / 7)\n",
    "        duration_df['day_cos'] = np.cos(2 * np.pi * duration_df['day_of_week'] / 7)\n",
    "        \n",
    "        # Add regime features\n",
    "        for regime in duration_df['regime'].unique():\n",
    "            duration_df[f'regime_{regime}'] = (duration_df['regime'] == regime).astype(int)\n",
    "            \n",
    "        # Add market condition features at regime start\n",
    "        for col in ['volatility_30s', 'trend_30s', 'bid_ask_spread', 'volume_imbalance_30s']:\n",
    "            duration_df[f'start_{col}'] = [labeled_df.loc[start, col] for start in duration_df['start_time']]\n",
    "        \n",
    "        return duration_df\n",
    "    else:\n",
    "        # If no durations found, create empty dataframe with expected columns\n",
    "        return pd.DataFrame(columns=['start_time', 'regime', 'duration', 'hour', 'minute', 'day_of_week',\n",
    "                                    'hour_sin', 'hour_cos', 'day_sin', 'day_cos'])\n",
    "\n",
    "def train_duration_model(duration_df):\n",
    "    \"\"\"Train a model to predict regime duration.\"\"\"\n",
    "    print(\"\\nTraining regime duration prediction model...\")\n",
    "    \n",
    "    if len(duration_df) < 10:\n",
    "        print(\"Not enough regime changes to train a duration model.\")\n",
    "        return None\n",
    "    \n",
    "    # Prepare features\n",
    "    feature_cols = [col for col in duration_df.columns if col not in ['start_time', 'regime', 'duration']]\n",
    "    X = duration_df[feature_cols]\n",
    "    y = duration_df['duration']\n",
    "    \n",
    "    # Train model\n",
    "    model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    # Calculate feature importance\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': feature_cols,\n",
    "        'importance': model.feature_importances_\n",
    "    }).sort_values(by='importance', ascending=False)\n",
    "    \n",
    "    return model, feature_importance\n",
    "\n",
    "# Create duration features and train model\n",
    "duration_df = create_duration_features(labeled_df)\n",
    "if not duration_df.empty and len(duration_df) > 10:\n",
    "    duration_model, feature_importance = train_duration_model(duration_df)\n",
    "    \n",
    "    # Display feature importance\n",
    "    print(\"\\nFeature Importance for Duration Prediction:\")\n",
    "    print(feature_importance.head(10))\n",
    "    \n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.barh(feature_importance['feature'][:10], feature_importance['importance'][:10])\n",
    "    plt.xlabel('Importance')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.title('Feature Importance for Regime Duration Prediction')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('duration_feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "else:\n",
    "    print(\"\\nNot enough regime changes to train a duration prediction model.\")\n",
    "    duration_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d2805300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating regime visualization over time...\n",
      "\n",
      "Creating 2D visualization of market regimes...\n"
     ]
    }
   ],
   "source": [
    "# 9. Visualization\n",
    "\n",
    "def visualize_regimes_over_time(labeled_df):\n",
    "    \"\"\"Visualize regime evolution over time with price overlay.\"\"\"\n",
    "    print(\"\\nCreating regime visualization over time...\")\n",
    "    \n",
    "    # Create figure for visualization\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(20, 12), sharex=True, gridspec_kw={'height_ratios': [3, 1]})\n",
    "    \n",
    "    # Plot mid-price\n",
    "    ax1.plot(labeled_df.index, labeled_df['mid_price'], color='black', alpha=0.7)\n",
    "    ax1.set_ylabel('Mid Price')\n",
    "    ax1.set_title('Market Price and Regime Evolution')\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # Plot regimes as background color\n",
    "    unique_regimes = sorted(labeled_df['cluster'].unique())\n",
    "    cmap = plt.cm.get_cmap('tab10', len(unique_regimes))\n",
    "    colors = {regime: cmap(i) for i, regime in enumerate(unique_regimes)}\n",
    "    \n",
    "    # Get regime change points\n",
    "    labeled_df['regime_change'] = labeled_df['cluster'] != labeled_df['cluster'].shift(1)\n",
    "    change_points = labeled_df[labeled_df['regime_change']].index.tolist()\n",
    "    change_points = [labeled_df.index[0]] + change_points + [labeled_df.index[-1]]\n",
    "    \n",
    "    # Plot regimes as background\n",
    "    for i in range(len(change_points) - 1):\n",
    "        start = change_points[i]\n",
    "        end = change_points[i+1]\n",
    "        regime = labeled_df.loc[start:end, 'cluster'].iloc[0]\n",
    "        ax1.axvspan(start, end, alpha=0.3, color=colors[regime])\n",
    "    \n",
    "    # Plot regime labels\n",
    "    ax2.scatter(labeled_df.index, labeled_df['cluster'], c=labeled_df['cluster'], cmap=cmap, s=5)\n",
    "    ax2.set_yticks(unique_regimes)\n",
    "    ax2.set_yticklabels([regime_characteristics[r]['name'] for r in unique_regimes], fontsize=10)\n",
    "    ax2.set_ylabel('Market Regime')\n",
    "    ax2.set_xlabel('Time')\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    # Format x-axis\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('regime_evolution.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def visualize_clusters_in_2d(reduced_features, labels):\n",
    "    \"\"\"Visualize clusters in 2D space using UMAP.\"\"\"\n",
    "    print(\"\\nCreating 2D visualization of market regimes...\")\n",
    "    \n",
    "    # Use UMAP for dimensionality reduction to 2D\n",
    "    reducer = umap.UMAP(random_state=42)\n",
    "    embedding = reducer.fit_transform(reduced_features.values)\n",
    "    \n",
    "    # Create visualization\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # Create a scatter plot\n",
    "    unique_labels = sorted(set(labels))\n",
    "    cmap = plt.cm.get_cmap('tab10', len(unique_labels))\n",
    "    \n",
    "    for i, label in enumerate(unique_labels):\n",
    "        mask = labels == label\n",
    "        plt.scatter(embedding[mask, 0], embedding[mask, 1], \n",
    "                   c=[cmap(i)], \n",
    "                   label=f\"{regime_characteristics[label]['name']}\")\n",
    "    \n",
    "    plt.title('Market Regimes Visualization in 2D Space')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('regime_clusters_2d.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# Run visualizations\n",
    "try:\n",
    "    visualize_regimes_over_time(labeled_df)\n",
    "except Exception as e:\n",
    "    print(f\"Error in regime visualization: {e}\")\n",
    "\n",
    "try:\n",
    "    visualize_clusters_in_2d(reduced_features, ensemble_labels)\n",
    "except Exception as e:\n",
    "    print(f\"Error in cluster visualization: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "34933efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing temporal patterns in regime occurrences...\n"
     ]
    }
   ],
   "source": [
    "# 10. Temporal Analysis for Regime Occurrences\n",
    "\n",
    "def analyze_temporal_patterns(labeled_df):\n",
    "    \"\"\"Analyze if certain regimes are more common during specific times of day.\"\"\"\n",
    "    print(\"\\nAnalyzing temporal patterns in regime occurrences...\")\n",
    "    \n",
    "    # Group by hour and regime\n",
    "    labeled_df['hour'] = labeled_df.index.hour\n",
    "    hour_regime_counts = labeled_df.groupby(['hour', 'cluster']).size().unstack(fill_value=0)\n",
    "    \n",
    "    # Normalize by hour\n",
    "    hour_regime_probs = hour_regime_counts.div(hour_regime_counts.sum(axis=1), axis=0)\n",
    "    \n",
    "    # Plot heatmap of regime probabilities by hour\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    sns.heatmap(hour_regime_probs, cmap='viridis', annot=True, fmt='.2f')\n",
    "    plt.title('Regime Probability by Hour of Day')\n",
    "    plt.xlabel('Regime')\n",
    "    plt.ylabel('Hour of Day')\n",
    "    plt.savefig('regime_hour_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    return hour_regime_probs\n",
    "\n",
    "# Run temporal analysis\n",
    "hour_regime_probs = analyze_temporal_patterns(labeled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a27d943e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== Market Regime Detection Summary ====\n",
      "Number of identified regimes: 2\n",
      "\n",
      "Regime Characteristics Summary:\n",
      "Regime 0: Trending Down & Liquid & Stable\n",
      "  - Descriptors: Trending Down, Stable, Liquid\n",
      "  - Order Book Imbalance: Balanced\n",
      "  - Avg Bid-Ask Spread: 1.301174\n",
      "  - Avg Volatility (30s): 0.000033\n",
      "  - Avg Return Trend (30s): -0.000000\n",
      "\n",
      "Regime 1: Mean-Reverting & Liquid & Stable\n",
      "  - Descriptors: Mean-Reverting, Stable, Liquid\n",
      "  - Order Book Imbalance: Balanced\n",
      "  - Avg Bid-Ask Spread: 1.301234\n",
      "  - Avg Volatility (30s): 0.000033\n",
      "  - Avg Return Trend (30s): 0.000000\n",
      "\n",
      "\n",
      "Most Common Regime Transitions:\n",
      "                               From                                To  \\\n",
      "1   Trending Down & Liquid & Stable  Mean-Reverting & Liquid & Stable   \n",
      "2  Mean-Reverting & Liquid & Stable   Trending Down & Liquid & Stable   \n",
      "0   Trending Down & Liquid & Stable   Trending Down & Liquid & Stable   \n",
      "3  Mean-Reverting & Liquid & Stable  Mean-Reverting & Liquid & Stable   \n",
      "\n",
      "   Probability  \n",
      "1          1.0  \n",
      "2          1.0  \n",
      "0          0.0  \n",
      "3          0.0  \n",
      "\n",
      "Optimal Trade Size by Regime:\n",
      "Trending Down & Liquid & Stable: Q25\n",
      "Mean-Reverting & Liquid & Stable: Q25\n",
      "\n",
      "Analysis Complete.\n"
     ]
    }
   ],
   "source": [
    "# 11. Summary and Final Analysis\n",
    "\n",
    "print(\"\\n==== Market Regime Detection Summary ====\")\n",
    "print(f\"Number of identified regimes: {len(regime_characteristics)}\")\n",
    "print(\"\\nRegime Characteristics Summary:\")\n",
    "for regime, details in regime_characteristics.items():\n",
    "    print(f\"Regime {regime}: {details['name']}\")\n",
    "    print(f\"  - Descriptors: {details['trend']}, {details['volatility']}, {details['liquidity']}\")\n",
    "    print(f\"  - Order Book Imbalance: {details['imbalance']}\")\n",
    "    \n",
    "    # Get statistics for key metrics\n",
    "    stats = details['stats']\n",
    "    print(f\"  - Avg Bid-Ask Spread: {stats['bid_ask_spread']:.6f}\")\n",
    "    print(f\"  - Avg Volatility (30s): {stats['volatility_30s']:.6f}\")\n",
    "    print(f\"  - Avg Return Trend (30s): {stats['trend_30s']:.6f}\")\n",
    "    print()\n",
    "\n",
    "# Display most common transitions between regimes\n",
    "print(\"\\nMost Common Regime Transitions:\")\n",
    "flat_transitions = transition_df.unstack().reset_index()\n",
    "flat_transitions.columns = ['From', 'To', 'Probability']\n",
    "top_transitions = flat_transitions.sort_values('Probability', ascending=False).head(5)\n",
    "print(top_transitions)\n",
    "\n",
    "# Display optimal trade size by regime (from market impact analysis)\n",
    "print(\"\\nOptimal Trade Size by Regime:\")\n",
    "for regime in impact_results['regime'].unique():\n",
    "    regime_impact = impact_results[impact_results['regime'] == regime]\n",
    "    optimal_size = regime_impact.loc[regime_impact['price_impact'].idxmin()]['trade_size_quantile']\n",
    "    print(f\"{regime}: {optimal_size}\")\n",
    "\n",
    "print(\"\\nAnalysis Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0615df2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "market_regime",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
